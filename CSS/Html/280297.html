<header><h1>How to configure volume mount points on a Microsoft Cluster Server</h1></header><div class='kb-summary-section section'> With the NTFS volume mount points feature, you can surpass the 26-drive-letter limitation. By using volume mount points, you can graft, or<br> <span class='sbody-italic'>mount</span> a target partition into a folder on another physical disk. volume mount points are transparent to programs. This article discusses how to create volume mount points on a server cluster, and considerations associated with it.<br> <br><br> Adding a mount point to shared disk is the same as adding a mount point to a non-shared disk. Mount points are added by Win32 API SetVolumeMountPoint, and are deleted by DeleteVolumeMountPoint. This has nothing to do with the disk resource dynamic link library (DLL). The resource DLL is only concerned about the volume global universal identifications (GUIDs), and not the actual mount points. <br><br> There are three ways to add mount points to a system (clustered and non-clustered are the same): <br> <ul class='sbody-free_list'><li>Logical Disk Manager (Diskmgmt.msc)</li><li>Mountvol.exe from the command prompt</li><li>Write your own .exe file, using the Win32 API SetVolumeMountPoint, and DeleteVolumeMountPoint</li></ul></div><div class='kb-moreinformation-section section'> When you create a volume mount point on a server cluster, you need to take into consideration the following key items in regards to volume mount points: <br> <ul class='sbody-free_list'><li>They cannot go between clustered, and non-clustered disks.</li><li>You cannot create mount points on the Quorum disk.</li><li>If you have a mount point from one shared disk to another, you must make sure that they are in the same group, and that the mounted disk is dependent on the root disk.</li></ul><h3 class='sbody-h3'>How to set up volume mount points on a Clustered Server</h3><ol class='sbody-num_list'><li>Log on locally with administrative rights to the node that owns the root disk, into which you are going to be grafting the directory. This is the disk that will contain the mount point.</li><li>Open Cluster Administrator (CluAdmin.exe), and pause other nodes in the Cluster.</li><li>Partition the disk, and then create the mount point. To do so, follow these steps: <br><ol class='sbody-alpha_list'><li>To open <span class='text-base'>Disk Management</span>, click <span class='text-base'>Start</span>, click <span class='text-base'>Run</span>, type <span class='sbody-userinput'>diskmgmt.msc</span>, and then click <span class='text-base'>OK</span>.</li><li>Select the disk that you would like to graft into the directory.</li><li>Right-click the free space on the disk, and then click <span class='text-base'>New Partition</span>.</li><li>Create a <span class='text-base'>Primary Partition</span>, and then click <span class='text-base'>Next</span>.</li><li>Set the size of the partition.</li><li>Select <strong class='uiterm'>Mount in the following empty NTFS folder</strong>, click <span class='text-base'>Browse</span> to browse to the directory in which you would like the mount point to be created, and then click <span class='text-base'>New Folder</span> (this will be the root into which the volume is mounted). Click the newly created folder, click <span class='text-base'>OK</span>, and then click <span class='text-base'>Next</span>.</li><li>Format the partition by using the NTFS File System.<br><br> This is a requirement of both Microsoft Cluster Server (MSCS), and the volume mount points feature.</li></ol></li><li>Create the new Disk resource, and then set dependencies. To do so, follow these steps: <br><ol class='sbody-alpha_list'><li>Open Cluster Administrator.</li><li>Right-click the group that owns the Shared Disk resource for the disk on which you just created the volume mount point. Click <span class='text-base'>New</span>, and then click <span class='text-base'>Resource</span>.</li><li>For the <span class='text-base'>Resource</span> type, click <span class='text-base'>Physical Disk</span>. Verify that it is in the same group as the the root disk. Click <span class='text-base'>Next</span>.</li><li>Make sure all nodes are possible owners, and then click<br> <span class='text-base'>Next</span>.</li><li>Double-click the root disk, to make this volume mount point disk dependent on the root disk. Click <span class='text-base'>Next</span>.</li><li>In the Disk Parameters window, you should see your disk listed. It will be listed by the disk number, and partition number; this is different from standard MSCS disks, which are listed by drive letter. Click <span class='text-base'>Finish</span>.</li><li>Right-click the new Disk resource, and then click <span class='text-base'>Bring Online</span>.</li></ol></li><li>Unpause all other nodes, and test that you can fail the group over to every node and access the newly created mount point.</li></ol><span class='text-base'>Important</span> The new volume mount point  functions on  all nodes in the cluster group. However, when you open Windows  Explorer or you double-click <strong class='uiterm'>My Computer</strong> on any node other than the node where the volume mount point was created, the new volume mount point may be displayed by using a folder symbol instead of by using a drive symbol. When you right-click the folder symbol and then click <strong class='uiterm'>Properties</strong>, the <strong class='uiterm'>File System</strong> value is set to <strong class='uiterm'>RAW</strong> and not to <strong class='uiterm'>NTFS</strong>. <br><br><br><br>To configure the volume mount point to display correctly on all  nodes in the cluster group, follow these steps.<br><br><span class='text-base'>Note</span> These steps must be performed on all the nodes that will own the volume mount point.<ol class='sbody-num_list'><li>As soon as the volume mount point has been created on node1, manually fail over to node2, and then pause all other nodes in the cluster except node2.</li><li>On node2, open Disk Management. To do this, follow these steps:<ol class='sbody-alpha_list'><li>Click <strong class='uiterm'>Start</strong>, click <strong class='uiterm'>Administrative Tools</strong>, and then click <strong class='uiterm'>Computer Management</strong>.</li><li>In the Computer Management MMC snap-in, click <strong class='uiterm'>Disk Management</strong>.</li></ol></li><li>In <strong class='uiterm'>Disk Management</strong>, right-click the mounted volume, and then click <strong class='uiterm'>Change Drive Letter and Paths</strong>.</li><li>Select the mount point, click <strong class='uiterm'>Remove</strong>,   click <strong class='uiterm'>Add</strong>, and then reassign the same drive letter to the mount point.</li><li>Unpause all other nodes.</li><li>Repeat steps 1 through 5 until the volume mount point is successfully created on all nodes in the cluster group.<br></li></ol><span class='text-base'>Note</span> After you follow these steps, the following conditions may continue to exist:<ul class='sbody-free_list'><li>The volume mount point is still displayed as a folder and not as a drive.</li><li>The <strong class='uiterm'>File System</strong> value is still set to <strong class='uiterm'>RAW</strong> and not to <strong class='uiterm'>NTFS</strong>.</li></ul>  However, the mount point continues to function correctly. This is a purely cosmetic issue. It is not a functional issue.</div><div class='kb-moreinformation-section section'>This behavior has been logged in the following bug: <div class='indent'>BUG #: <a href='http://bugcheck/bugs/WINSE/216357.asp' title='Link only available for Microsoft corporate network users' target='_blank'>216357 (WINSE)</a></div></div><div class='kb-moreinformation-section section'><h3 class='sbody-h3'>Best practices when you use volume mount points</h3>Some best practices for when you are using volume mount points are as follows:<ul class='sbody-free_list'><li>Try to use the root (host) volume exclusively for mount points. The root volume is the volume that is hosting the mount points.  This greatly reduces the time that it takes to restore access to the mounted volumes if you have to run a chkdsk. This also reduces the time that it takes to restore from backup on the host volume.</li><li>If you use the root (host) volume exclusively for mount points, the size of the host volume only has to be several MB. This reduces the probability that the volume is used for anything other than the mount points. </li><li> In a cluster, where high availability is important, you can make redundant mount points on separate host volumes. This helps guarantee that if one root (host) volume is inaccessible, you can still access the data that is on the mounted volume through the other mount point. For example, if HOST_VOL1 (D:) is on Mountpoint1, user data is on LUN3. Then, if  HOST_VOL2 (E:) is on Mountpoint1, user data is on LUN3. Therefore, customers can now access LUN3 through either D:\mountpoint1 or through E:\mountpount1.<br><br><span class='text-base'>Note</span> Because the user data that is on LUN3 depends on both D: and E: volumes, you have  to temporarily remove the dependency of any failed host volume until the volume is back in service.  Otherwise, the user data that is on LUN3 remains in a failed state.</li></ul></div><div class='kb-securedata-section section'> This is how volume mount points work:<br><br> The only thing that Cluster does is make sure that all nodes in the Cluster have the same list of volume GUIDs for the shared disks. The reason for this is that Windows 2000 could assign different GUIDs for each disk on each node. MSCS does not actually do anything with mount points directly. The mount points are stored in the file system, and they work by mounting the volume with the specified GUID. The issue is that if the nodes each have a different GUID for the volume, and then you create a mount point on one node, when you failover the file system on the other node, it will not have a clue which volume the mount point is referring to because the GUIDs do not match up. Fortunately, each volume can have multiple GUIDs, so we just add the first node&#39;s volume GUID to the volume before you put the disk online, at which point, everything works as expected.<br><br> When a disk comes online, you get the volume GUID for the disk, and then add it to the VolGUID table. If someone creates a mount point on the disk, there is a connection made by the file system between that volume GUID and the mount point. The disk resource DLL does not care about the reparse data because we already have the volume GUID stored for this disk. So when someone accesses the mount point on the disk, the file system uses the volume GUID to determine which physical disk to actually get the data from.<br><br> When a disk moves to another node, you parse the VolGUID table, and set each volume GUID for that disk on that node. That means that one partition might have more than one volume GUID assigned. When someone accesses that mount point that had the VolGUID from node1, the VolGUID from node 1 is also set on node 2 so the mount point works.<br><br> The catch in all this is that the partmgr/volmgr now tries to assign the VolGUID for a disk if it can figure out the mount point when bringing the disk online. So disk X might have the same VolGUID on every system on which it is running. On Windows 2000, the VolGUID for disk X was always different for every node, but this is no longer true for Windows Server 2003. If partmgr/volmgr assigns the same VolGUID, the disk the reparse point data on the disk just works without disk resource DLL doing anything. If partmgr/volmgr does not assign the VolGUID, the disk resource DLL will make sure that the VolGUID list is assigned for each disk. Here is an example of how it works:<br><br> Node 1: <div class='indent'>  disk X has volguidx1 - assigned by partmgr/volmgr<br>  disk Y has volguidy1 - assigned by partmgr/volmgr<br></div> Disk resource saves: <div class='indent'>  X has VolGUID list: volguidX1<br>  Y has VolGUID list: volguidY1<br></div><br> Create mount point from X to Y. On disk X, there is a reparse point that says volguidY1.<br><br> Move disks to node 2: <div class='indent'>  disk X has volguidx2 - assigned by partmgr/volmgr<br>  disk Y has volguidy2  - assigned by partmgr/volmgr<br></div> Disk resource saves: <div class='indent'>  X has VolGUID list: volguidx1, volguidx2<br>  Y has VolGUID list: volguidy1, volguidy2<br></div> Disk resource assigns all VolGUIDs in the list to the appropriate device. So, on node 2, disk X has both volguidX1 and volguidX2. Disk Y has both volguidY1, and volguidY2.<br><br> So when someone tries to access the mount point on disk X, it says to go to volguidY1. On node 2, there was no volguidY1 until the disk resource DLL assigned it to disk Y. Now the mount point on disk X works on node 2.<br><br> If disks are moved back to node 1, all volguids are assigned on node 1 as they are on node 2. </div><div class='kb-references-section section'><span>For more information about how to create volume mount points from a command prompt, click the following article number to view the article in the Microsoft Knowledge Base:<br><br><div class='indent'><a id='kb-link-1' href='/en-us/help/205524'>205524 </a> How to create and manipulate NTFS junction points<br><br></div></span><span>For more information about how drives are handled by the operating system, click the following article number to view the article in the Microsoft Knowledge Base:<br><br><div class='indent'><a id='kb-link-2' href='/en-us/help/234048'>234048 </a> How Windows 2000 assigns, reserves, and stores drive letters<br><br></div></span><span>For more information about volume mount points and SQL 2000, click the following article number to view the article in the Microsoft Knowledge Base:<br><br><div class='indent'><a id='kb-link-3' href='/en-us/help/819546'>819546 </a> SQL Server 2000 support for mounted volumes<br><br></div></span><span>For more information about volume mount points and Exchange Server 2003, click the following article number to view the article in the Microsoft Knowledge Base:<br><br><div class='indent'><a id='kb-link-4' href='/en-us/help/318458'>318458 </a> Volume mount point support for an Exchange Server 2003 cluster on a Windows Server 2003-based system<br><br></div></span></div><div class='kb-securedata-section section'>kbclustering MSCS Diskmgmt msc Mountvol exe CluAdmin</div><div class='kb-textlegacy-section section'>Author: ELDENC (2000-11-20T12:01:00) Edit Reviewer: V-RROYER (2001-02-06T17:52:00) Tech Reviewer: johnmar (2001-01-26T14:28:00)<br> </div><div class='kb-securedata-section section'>Rewrite Writer: v-lgomm<br>Rewrite Tech Reviewer: v-jomcc<br>Rewrite Editor: v-crweb</div>